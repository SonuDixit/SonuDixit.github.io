<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects - Sonu Dixit</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Sonu Dixit</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <!-- <li><a href="projects.html">Projects</a></li> -->
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>
    <section>
        <h2 id="llm_finetuning">LLM finetuning </h2>
        <ol>
            <li>
                <strong>Dataset Prep:</strong> x, y = whole conversation, but We calculate loss on the Agent turns. In y, We replace other tokens with -100(pytorch specific) to ignore cross entropy loss calculation.
                <ul>
                    <!-- <li><strong>add_generation_prompt = true</strong></li> -->
                    <li>We put the masking code as part of tokenisation. I made a mistake, which I realized 2 days later when I was getting poor results - all the generated tokens were |EOT|.</li>
                </ul>
            </li>
            <li>On an 80GB GPU, I could fit only one chat, making batch size = 1.</li>
            <li>DDP training on 2 x 80GB GPUs, with a gradient accumulation step of 128.</li>
            <!-- <li>Train ~ 25000, val ~5000, test ~30000.</li> -->
            <li>First experiment was to overfit 20 chats, with the last classification layer training and/or last decoder layer training. We found the model could completely overfit the data.</li>
            <li>Trained only the last layer, with learning rate in range of (1e-5 to 1e-6).</li>
            <li>
                Validation on validation data of about 5000 chats:
                <ul>
                    <li>Was calculating CE loss and writing some samples in a different file for manual review.</li>
                </ul>
            </li>
            <li>
                <strong>Result:</strong>
                <ul>
                    <li><strong>Its performance improved:</strong>
                        <ul>
                            <li>Prior to fine-tuning, it used to generate verbose output; after, it started generating in Agent tone, short-length outputs.</li>
                            <li>Before fine-tuning, it wasn't personalizing the conversation with the customer's name and wasn’t using its own name (like Agent). After fine-tuning, it started to use the customer's name to personalize the conversation and also generated an Agent name. This may have been possible through prompt engineering.</li>
                            <li>Its performance improved significantly in initial and end turns.</li>
                            <li>In middle turns, where it needs external information, it was not able to generate relevant responses.</li>
                            <li>Its performance degraded on the Instruction-following benchmark. One reason for this is, in this training dataset, all the chats had the same instruction: “you are a helpful chatbot. Knowledge cut-off date - date.” The training data had no variation. I recommend training on a mixture of data; in this case, it could be 70% chat data, and the rest 30% could include other data (general instruction following, math, English, reasoning, medical, etc.).</li>
                        </ul>
                    </li>
                </ul>
            </li>
        </ol>

        <h2 id="intent_classification"> Intent Classification </h2>
        <ol>
            <li> Classes are added/removed with time. </li>
            <li> We finetune transformer based model on our data. We find manual finetuning to be better than <a href="https://arxiv.org/abs/2209.11055">SetFit</a></li>
            <li> We compare against llms (GPT, Mixtral, and others) using few shot prompt-tuning and finetuning(generation based classification) </li>
            <li> We conduct experiments to estabilish the approach can understand client-useful words even when its OOV for the model. </li>
            <li> We conduct experimens to study its performance on OOD data, and how to minimise the risk. </li>
            <li> We found bias in models behaviour. We design solutions to handle such bias. </li>

        </ol>

        <h2 id="info_retrieval"> Information Retrieval and Generation </h2>
        <ol>
            <li> We train multiple formulations Bert_dot, Bert_cat, ColBert for ranking task. Bert_cat outperforms other formulations.</li>
            <li> Bert_cat requires a lot of inference time compute. To reduce latency We distil from ensemble scores of Bert_cat to DistilBERT</li>
            <li> We finetune the trained DistilBERT on our client specific data. </li>
            <ul>
                <li> Recommendation from a database of Question-Answer pairs. Upon finetuning, We outperform the exisiting solution. </li>
                <li> Conditional response generation for an ongoing conversation. Results on public data <a href="https://eval.ai/web/challenges/challenge-page/689/leaderboard/1909/Recall%25405"> team_name-Test_Team_Name </a>  </li>
                <ul>
                    <li> End to end training of RAG system using posterior guided retriever </li>
                    <li> We manually evaluate the trained system on a client specific data (web-scraped data and questions) </li>
                </ul>    
            </ul>
            <!-- <li> </li> -->

        </ol>

    </section>
    <!-- <footer>
        <p>&copy; 2024 Sonu Dixit. All rights reserved.</p>
    </footer> -->
</body>
</html>
