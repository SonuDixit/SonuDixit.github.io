<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sonu Dixit</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Sonu Dixit</h1>
        <nav>
            <ul>
                <li><a href="#about">About</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#publications">Publications</a></li>
                <!-- <li><a href="#education">Education</a></li> -->
                <!-- <li><a href="#projects">Projects</a></li> -->
                <!-- <li><a href="paper_reading_summary.html">Paper Reading Summary</a></li> -->
                <li><a href="#contact">Contact</a></li>
                <li><a href="cv.html">CV</a></li>
            </ul>
        </nav>
    </header>
    <div class="intro", id="about">
        <h2>About Me </h2>
        <div class="content">
        <p>Hello! I like to apply maths and neural networks to solve real-world problems. 
            I have a strong foundation in AI and hands-on experience across diverse domains. I have worked on LLM fine-tuning, large-scale information retrieval, question-answering, and reinforcement learning. 
            Please refer to the experience section for more details on my projects.  
            <br><br> I earned my Master's degree in Artificial Intelligence from the Indian Institute of Science in 2019.
            During my Masters, I worked on multi-agent reinforcement learning for traffic signal control.
            In the industry, I have contributed to [24]7.ai and Disney Star India Pvt Ltd.
            <br><br> Outside of work, I enjoy swimming, running, and spending time in nature through long walks and treks.</p>
        <img src="images/sonu_pic.jpeg" alt="Sonu Dixit">
        </div>
    </div>

    <section id="experience">
        <h2>Industry Experience</h2>
        <h3> LLama 3 Finetuing for Customer Chat Conversation </h3>
        <ul>
            <li>We finetune LLama 3.1 8B model on Agent Customer conversation data.</li>
            <!-- <li><strong>Result</strong> -->
            <li>After fine-tuning, the model shifted from generating verbose responses to concise, agent-like replies.</li>
            <li>It began personalizing conversations by using customer names and adopting an agent identity, which was absent before fine-tuning. This effect might also be achievable through prompt engineering.</li>
            <li>Performance significantly improved in initial and final conversation turns.</li>
            <li>However, the model struggled to generate relevant responses during middle turns that required external information.</li>
            <li>Performance on instruction-following benchmarks declined, likely due to the lack of diversity in training instructions. To address this, I recommend using a mixed dataset—70% chat data and 30% diverse tasks (e.g., instruction-following, reasoning, math, etc.).</li>
            <!-- </li> -->
        </ul>
        
        <h3> Intent Classification </h3>
        <ul>
            <li>We classify customer queries into predefined intents to enable automated handling by our Chatbot. Our trained model outperforms the existing solution and LLMs including GPT-3.5.</li>
            <li>Transformer Based multiple network(s) training. </li>
            <li>Classes are added/removed with time. Final solution design must take this into account.</li>
            <li>We find manual finetuning to be better than data efficient methods like <a href="https://arxiv.org/abs/2209.11055">SetFit</a></li>
            <li>We compare against llms (GPT, Mixtral, and others) using few shot prompt-tuning and finetuning(generation based classification) </li>
            <li>We design and conduct experiments to estabilish the approach can understand client-useful words even when its OOV for the model. </li>
            <li>We conduct experimens to study its performance on OOD data, and how to minimise the risk. </li>
            <li>We found bias in models behaviour. We design solutions to handle such bias. </li>

        </ul>
        
        <h3>Information Retrieval and Conditional Generation </h3>
        <ul>
            <li>Trained a DistilBERT on the MSMarco dataset to proactively recommend relevant articles, achieving comparable performance on the <a href="https://microsoft.github.io/msmarco/TREC-Deep-Learning-2021">TREC21</a> and TREC20 datasets.</li>
            <li>We train multiple formulations Bert_dot, Bert_cat, <a href="https://arxiv.org/abs/2004.12832">ColBert</a> for ranking task. Bert_cat outperforms other formulations.</li>
            <li>Bert_cat requires a lot of inference time compute. To reduce latency We perform a dual distil from ensemble scores of Bert_cat and ColBert to the DistilBERT</li>
            <!-- <li>Distillation, Multiple methods of scoring (query, passage) pair, tradeoff in scoring, represetantion mechanism and final performance </li> -->
            <li>The method is inspired from <a href="https://arxiv.org/abs/2104.06967"> Efficiently teaching an effective dense retriever with balanced topic aware sampling</a></li>
            <li>We explore multiple <a href="https://arxiv.org/abs/1702.08734">first level rankers</a> (TF-IDF, BM25, LSH, PQ-Encoding)</li>
            <li> We finetune the trained DistilBERT on our client specific data. </li>
            <ul>
                <li> Recommendation from a database of Question-Answer pairs. Upon finetuning, We outperform the exisiting solution. </li>
                <li> Conditional response generation for an ongoing conversation. Results on public data <a href="https://eval.ai/web/challenges/challenge-page/689/leaderboard/1909/Recall%25405"> team_name-Test_Team_Name </a>  </li>
                <ul>
                    <li> End to end training of RAG system using <a href="https://arxiv.org/abs/2110.07752">posterior guided retriever</a>. Here we use the trained DBert as the retriever. </li>
                    <li> We manually evaluate the trained system on a client specific data (web-scraped client website data and questions) </li>
                </ul>    
            </ul>
        </ul>
        
        <!-- <h3>Answer Recommendation, Generation</h3>
        <ul>
            <li>Recommended relevant queries and answers from a Knowledge Base, outperforming existing methods.</li>
            <li>Generate answer conditioned on recommendations, inspired from <a href="https://arxiv.org/abs/2110.07752"> Posterior guided training for retrievers</a> and <a href="https://arxiv.org/abs/1811.01241">Knowledge powered conversational agents </a></li>
            <li>End to end training of RAG systems</li>
        </ul> -->
        
        <h3>LLM as Co-Pilot</h3>
        <ul>
            <li>We utilize LLMs to generate contextually relevant responses during conversations.</li> 
            <li>We append the LLM context with relevant historical information, using the retriever trained in the IR task.</li>
            <li>We designed an identity task <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">(similar to Needle in a Haystack)</a> to evaluate the LLM performance.</li>
        </ul>

        <h3>Compliance Evaluation and Early Warning System - Agent-Customer Chat</h3>
        <ul>
            <li>We pass the quantifiable metrics to LLM. </li>
            <li>Prompt-Engineering, We use LLM-as-judge for evaluation. </li>
            <!-- <li>We use LLMs to generate warnings for non-compliance content. </li> -->
        </ul>

        <h3>Named Entity Recognition by Question Answering</h3>
        <ul>
            <li>Different clients need different granular entities(start_date_of_reservation, flight_time, ..). Traditional NER requires training data for the specific entities. </li>
            <li>We formulate this as QA problem, where question is asked for client specific entities. Model needs to Generate/extract the answer(entity). The same underlying QA-system can generalise to multiple clients. </li>
            <li>It reduces client-specific entity extraction finetuning. </li>
        </ul>

        <h3> Content Analysis and Storyline Extraction</h3>
        <ul>
            <li>Quantified character attributes using text, image, and audio data, correlating features with viewership metrics.</li>
            <li>Episode vector learning, social relationship, face recognition. <a href="https://sites.google.com/view/contentunderstanding/home">Details</a></li>
        </ul>
        <h3>Entity extraction from Semi-Structured Documents</h3>
        <ul>
            <li> Used HMM to extract values corresponding to predefined keys from documents.</li>
            <li> Implemented the Viterbi and the scaled Baum-Welch algorithm for learning parameters.</li>
        </ul>

    </section>
    <!-- <section id="publications">
        <h2>Publications</h2>
        <ul>
            <li>R Hazra*, <u>Sonu Dixit*</u> ,S Sen, <a href="https://vigilworkshop.github.io/static/papers-2021/13.pdf">NAACL 2021 workshop - ViGIL</a>, <a href="https://arxiv.org/pdf/2012.05011">Intrinsically Motivated Compositional Lnguage Emergence </a>,   <a href="https://sites.google.com/view/compositional-comm">Website</a></li>
            <li>R Hazra, <u>Sonu Dixit</u>, NAACL 2021 workshop - ViGIL <a href="https://vigilworkshop.github.io/static/papers-2021/16.pdf">gComm: An environment for investigating generalization in Grounded Language Acquisition</a>  </li>
        </ul>
    </section> -->

    <section id="publications">
        <h2>Publications</h2>
    
        <div class="publication">
            <img src="images/intrinsic_reward/compositionality_image.gif" alt="Publication 1 Image"> 
            <div class="publication-content">
                <!-- <h3><a href="https://arxiv.org/abs/2012.05011">Intrinsically Motivated Compositional Language Emergence</a></h3> -->
                <h3><a href="https://vigilworkshop.github.io/static/papers-2021/13.pdf">Zero-Shot Generalization using Intrinsically Motivated Compositional Emergent Protocols</a></h3>
                <p><strong>Authors:</strong> R Hazra*, <u>Sonu Dixit*</u> — NAACL 2021 Workshop (ViGIL)</p>
                <p>Latest Preprint:<a href="https://arxiv.org/abs/2012.05011">Intrinsically Motivated Compositional Language Emergence</a></p>
                <p>We argue intrinsic rewards increase compositionality in emergent communication between two Agents. The improved compositionality increases zero shot generalisation in the downstream tasks.
                    <a href="https://sites.google.com/view/contentunderstanding/home">website</a>
                </p>
                <!-- <p></p> -->
            </div>
        </div>
    
        <div class="publication">
            <img src="images/gcomm/gcomm_env_image.png" alt="Publication 2 Image">
            <div class="publication-content">
                <h3><a href="https://vigilworkshop.github.io/static/papers-2021/16.pdf">gComm: Investigating Generalization in Grounded Language Acquisition</a></h3>
                <p><strong>Authors:</strong> R Hazra, <u>Sonu Dixit</u>, S Sen — NAACL 2021 Workshop (ViGIL)</p>
                <p>This work introduces gComm, an environment designed to evaluate grounded language acquisition. We focus on how agents generalize language understanding in complex, multi-agent scenarios.</p>
            </div>
        </div>
    </section>
    

    <!-- <section id="education">
        <h2>Education</h2>
        <p><strong>Indian Institute of Science (2017 – 2019)</strong><br> Masters - MTech (Artificial Intelligence), Overall GPA: 7.5/10</p>
        <p><strong>Gurukula Kangri University (2012 – 2016)</strong><br> Bachelors - BTech (Computer Science), Overall percentage: 78.60</p>
    </section> -->

    <section id="thesis">
        <h2>M.Tech Thesis: Adaptive Traffic Signal Control using Multi Agent RL</h2>
    
        <div class="thesis">
            <img src="images/traffic/marl_project_result.png" alt="M.Tech Thesis Image">
            <div class="thesis-content">
                <!-- <h3>Adaptive Traffic Signal Control Using Multi-Agent Reinforcement Learning</h3> -->
                <!-- <p>My thesis focuses on optimizing urban traffic flow using multi-agent reinforcement learning (MARL). By implementing the Proximal Policy Optimization (PPO) algorithm, we dynamically adjusted traffic signals based on real-time congestion data, significantly improving traffic efficiency compared to traditional fixed-time controls.</p> -->
                <ul>
                    <li>Implemented multi-agent reinforcement learning (MARL) to dynamically adjust traffic signal durations based on congestion.</li>
                    <li>Algorithm: Proximal Policy Optimization (PPO) with Advantage Actor-Critic. <a href="https://github.com/SonuDixit/Traffic/blob/master/mtech_project_report_results/report.pdf">Report</a></li>
                    <li>Simulated traffic data using PTV Vissim and demonstrated MARL’s superiority over fixed-time algorithms in terms of average speed, delay, and lane occupancy. <a href="https://github.com/SonuDixit/Traffic/blob/master/mtech_project_report_results/mtech_project.pdf">Results</a></li>
                    <li>Each Signal is represented as a Neural Network. During training multiple small neural networks are trained at same time. The networks are connected via the reward that they get.</li>
                    <li>Advisor - <a href="https://www.csa.iisc.ac.in/~shalabh/">Prof Shalabh Bhatnagar</a> </li>
                </ul>
            </div>
        </div>
    </section>

    <section id="contact">
        <h2>Contact</h2>
        <p>Email: <a href="mailto:sonudixit2k@gmail.com">sonudixit2k@gmail.com</a></p>
        <p>LinkedIn: <a href="https://www.linkedin.com/in/sonudixit/">linkedin.com/in/sonudixit</a></p>
    </section>
    <!-- <footer>
        <p>&copy; 2024 Sonu Dixit. All rights reserved.</p>
    </footer> -->
</body>
</html>
